import os
import json
import re
import requests # ğŸ“¦ ë„¤ì´ë²„ API í˜¸ì¶œì„ ìœ„í•´ ì¶”ê°€
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from time import time, sleep
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
# í”„ë¡¬í”„íŠ¸ ì„¤ì • ë¡œë“œ
from config import INTEGRITY_SCANNER_PROMPT, BRIDGE_CONCEPT_BATCH_PROMPT, LOGIC_FLOW_CHECK_PROMPT, CREATIVE_CONNECTION_BATCH_PROMPT

# --------------------------------------------------------------------------------------
# --- 1. ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ ---
# --------------------------------------------------------------------------------------

# [ë„¤ì´ë²„ API ì„¤ì •]
# [ì„¤ì • ë³€ê²½]
# NAVER_GATEWAY_KEYëŠ” ì´ì œ ì‚­ì œí•˜ì…”ë„ ë©ë‹ˆë‹¤.
NAVER_CLOVA_URL = os.environ.get('NAVER_CLOVA_URL2') # "https://clovastudio.stream..."
NAVER_API_KEY = os.environ.get('NAVER_API_KEY')     # "nv-...." (ìƒˆë¡œ ë°œê¸‰ë°›ì€ í‚¤)

# S-BERT ì„¤ì • (ìœ ì§€)
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
embedding_model = None

try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print(f"[Service Deep Analysis] Embedding Model loaded.")
except Exception as e:
    print(f"[Service Deep Analysis] CRITICAL: Embedding Model Failed: {e}")

# --------------------------------------------------------------------------------------
# --- 2. í—¬í¼ í•¨ìˆ˜ (Naver HyperCLOVA X í˜¸ì¶œ) ---
# --------------------------------------------------------------------------------------
def _call_llm_json(prompt_text):
    """
    [ìµœì‹ ] Naver HyperCLOVA X API í˜¸ì¶œ (Bearer Token ë°©ì‹)
    """
    if not NAVER_CLOVA_URL or not NAVER_API_KEY:
        print("âš ï¸ Naver API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # [í•µì‹¬ ë³€ê²½] í—¤ë”ê°€ ì•„ì£¼ ì‹¬í”Œí•´ì¡ŒìŠµë‹ˆë‹¤.
    headers = {
    'Authorization': f'Bearer {NAVER_API_KEY}',
    'Content-Type': 'application/json; charset=utf-8',
    'Accept': 'application/json'   # <--- ìˆ˜ì •! (ì™„ì„±ëœ JSONìœ¼ë¡œ ë‹¬ë¼ëŠ” ëœ»)
    }

    # HyperCLOVA X ìš”ì²­ íŒŒë¼ë¯¸í„° (ë©”ì‹œì§€ êµ¬ì¡°ëŠ” ë™ì¼)
    data = {
        "messages": [
            {
                "role": "system",
                "content": "ë„ˆëŠ” ë…¼ë¦¬ì ì¸ í•™ìˆ  ë©˜í† ì•¼. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í¬ë§·ìœ¼ë¡œë§Œ ì¶œë ¥í•´. ë§ˆí¬ë‹¤ìš´ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¤˜."
            },
            {
                "role": "user",
                "content": prompt_text
            }
        ],
        "topP": 0.8,
        "topK": 0,
        "maxCompletionTokens": 2048,
        "temperature": 0.2,
        "repeatPenalty": 1.5,
        "stopBefore": [],
        "includeAiFilters": True,
        "seed": 0
    }

    
    try:
        response = requests.post(NAVER_CLOVA_URL, headers=headers, json=data, stream=False)
        response.raise_for_status()
        
        res_json = response.json()
        content_text = res_json.get('result', {}).get('message', {}).get('content', '')
        
        if not content_text:
            print(f"[Naver] Empty content received.")
            return None

        # --- [JSON ì¶”ì¶œ ë° íŒŒì‹± ë¡œì§ ê°•í™”] ---
        
        # 1. Markdown ì½”ë“œë¸”ë¡ ì œê±°
        json_str = ""
        match = re.search(r"```json\s*([\s\S]+?)\s*```", content_text)
        if match:
            json_str = match.group(1)
        else:
            # ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ì¶”ì¶œ ì‹œë„
            json_match = re.search(r"(\{[\s\S]*\}|\[[\s\S]*\])", content_text.strip())
            json_str = json_match.group(1) if json_match else content_text.strip()

        # 2. íŒŒì‹± ì‹œë„ (3ë‹¨ê³„ ë°©ì–´ ì „ëµ)
        
        # [1ì°¨ ì‹œë„] í‘œì¤€ json.loads (strict=False)
        try:
            return json.loads(json_str, strict=False)
        except json.JSONDecodeError:
            pass # ì‹¤íŒ¨ ì‹œ 2ì°¨ ì‹œë„ë¡œ ë„˜ì–´ê°

        # [2ì°¨ ì‹œë„] ast.literal_eval (Python êµ¬ì¡° íŒŒì‹±)
        # LLMì´ ê°€ë” JSON ëŒ€ì‹  Python Dict í˜•íƒœ(True/False, ì‹±ê¸€ì¿¼íŠ¸ ë“±)ë¥¼ ì¤„ ë•Œ ìœ ìš©í•¨
        try:
            return ast.literal_eval(json_str)
        except:
            pass

        # [3ì°¨ ì‹œë„] í”í•œ ì˜¤ë¥˜(ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ìŒë”°ì˜´í‘œ) ìˆ˜ë™ ìˆ˜ì • í›„ ì¬ì‹œë„
        try:
            # "quote": "..." íŒ¨í„´ ì•ˆì˜ ë‚´ìš©ë¬¼ì€ ê±´ë“œë¦¬ì§€ ì•Šê³ , êµ¬ì¡°ë¥¼ ë§ê°€ëœ¨ë¦¬ëŠ” ê²ƒë§Œ ìˆ˜ì •í•˜ê¸´ ì–´ë µì§€ë§Œ
            # ë‹¨ìˆœí•˜ê²Œ ì¤„ë°”ê¿ˆ ë¬¸ì œì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
            json_str_clean = json_str.replace('\n', '\\n').replace('\r', '')
            return json.loads(json_str_clean, strict=False)
        except:
            pass
            
        print(f"[JSON Parsing Failed] Content: {content_text[:200]}...")
        return None

    except Exception as e:
        print(f"[Naver API Error] {e}")
        return None

# --- (ì•„ë˜ S-BERT ê´€ë ¨ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€) ---

def _get_embedding(text):
    if not embedding_model: return None
    return embedding_model.encode(text)

def _calculate_similarity(text_a, text_b):
    vec_a = _get_embedding(text_a)
    vec_b = _get_embedding(text_b)
    if vec_a is None or vec_b is None: return 0.0
    return float(cosine_similarity([vec_a], [vec_b])[0][0])

def extract_representative_sentences(text_sentences, query_summary, top_k=1):
    if not text_sentences or not embedding_model: return ""
    sentence_embeddings = embedding_model.encode(text_sentences)
    query_embedding = embedding_model.encode(query_summary)
    similarities = cosine_similarity([query_embedding], sentence_embeddings)[0]
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return text_sentences[top_indices[0]] if len(top_indices) > 0 else ""

# --------------------------------------------------------------------------------------
# --- 3. í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ (ë¡œì§ì€ ìœ ì§€í•˜ë˜, ìˆœì°¨ ì²˜ë¦¬ëŠ” API ì œí•œì— ë”°ë¼ ì¡°ì •) ---
# --------------------------------------------------------------------------------------
def analyze_logic_neuron_map(text, key_concepts_str, core_thesis):
    """
    [Zone ê¸°ë°˜ ê³ ë„í™”] ë…¼ë¦¬ ë‰´ëŸ° ë§µ ìƒì„± (Full Batch Optimization)
    - LLM í˜¸ì¶œì„ ë‹¨ 2íšŒ(Zone C 1íšŒ + Bridge 1íšŒ)ë¡œ ìµœì†Œí™”í•˜ì—¬ ì†ë„ ìµœì í™”
    """
    start_time = time()
    print("ğŸš€ [Neuron Map] (Naver/Batch) ë¶„ì„ ì‹œì‘.")
    
    # 0. ê¸°ë³¸ ë°ì´í„° ê²€ì¦
    if not key_concepts_str: 
        return {"nodes": [], "edges": [], "suggestions": [], "creative_feedbacks": []}
    
    concepts = [c.strip() for c in key_concepts_str.split(',') if c.strip()]
    if not concepts: 
        return {"nodes": [], "edges": [], "suggestions": [], "creative_feedbacks": []}

    nodes = [{"id": c, "label": c} for c in concepts]
    edges = []
    
    # ìƒíƒœ ì¶”ì  ë³€ìˆ˜
    connected_status = {c: False for c in concepts}
    potential_bridges = {} # Zone B ì €ì¥ìš©
    
    # [ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì»¨í…Œì´ë„ˆ]
    zone_c_candidates = [] # [{'id': 0, 'source': 'A', 'target': 'B', 'context': '...'}, ...]
    bridge_candidates = [] # [{'id': 0, 'iso': 'A', 'partner': 'B'}, ...]
    
    paragraphs = [p for p in text.split('\n') if len(p) > 20]

    # 1. S-BERT Batch Encoding
    if embedding_model:
        concept_vectors = embedding_model.encode(concepts) 
    else:
        concept_vectors = [None] * len(concepts) 

    # 2. Pairwise ë¶„ì„ (N x N)
    print("   [Neuron Map] Pairwise ê³„ì‚° ë° Zone ë¶„ë¥˜...")
    zone_c_idx = 0 # Zone C ë°°ì¹˜ ID ì¹´ìš´í„°

    for i in range(len(concepts)):
        for j in range(i + 1, len(concepts)):
            c1 = concepts[i]
            c2 = concepts[j]
            
            # (A) ë¬¼ë¦¬ì  ê±°ë¦¬ ê³„ì‚°
            physical_score = 0.0
            context_sent = ""
            for p in paragraphs:
                if c1 in p and c2 in p:
                    physical_score += 1.0
                    if not context_sent: context_sent = p # ì²« ë°œê²¬ ë¬¸ì¥ ì €ì¥
            physical_score = min(physical_score / 2.0, 1.0)

            # (B) ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚°
            if concept_vectors[i] is not None:
                semantic_score = float(cosine_similarity([concept_vectors[i]], [concept_vectors[j]])[0][0])
            else:
                semantic_score = 0.0
            
            # --- Zone íŒë³„ ë° ì—£ì§€ ìƒì„± ---
            
            # Case 1: Zone C (ì°½ì˜ì /ì–µì§€ ì—°ê²° ì˜ì‹¬) -> ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if semantic_score < 0.2 and physical_score >= 0.5:
                edges.append({
                    "source": c1, "target": c2, 
                    "weight": round(semantic_score, 2),
                    "type": "questionable" # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ì„ /ë¬¼ê²°ì„  í‘œì‹œ
                })
                connected_status[c1] = True
                connected_status[c2] = True
                
                if context_sent:
                    zone_c_candidates.append({
                        "id": zone_c_idx,
                        "source": c1,
                        "target": c2,
                        "context": context_sent[:200] # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
                    })
                    zone_c_idx += 1

            # Case 2: Zone B (ì ì¬ì  ì—°ê²°) -> ë‚˜ì¤‘ì— Bridge í›„ë³´ë¡œ ì‚¬ìš©
            elif semantic_score > 0.55 and physical_score < 0.2:
                potential_bridges[(c1, c2)] = semantic_score

            # Case 3: Zone A (ì¼ë°˜ì  ê°•í•œ ì—°ê²°)
            else:
                total_weight = (physical_score * 0.3) + (semantic_score * 0.7)
                if total_weight >= 0.3:
                    edges.append({
                        "source": c1, "target": c2, 
                        "weight": round(total_weight, 2),
                        "type": "strong" if total_weight > 0.55 else "normal"
                    })
                    connected_status[c1] = True
                    connected_status[c2] = True

    # 3. ì™¸ë”´ ì„¬(Isolated Node) Bridge í›„ë³´ ì„ ì •
    isolated_nodes = [node for node, connected in connected_status.items() if not connected]
    processed_iso_nodes = set()
    bridge_idx = 0 # Bridge ë°°ì¹˜ ID ì¹´ìš´í„°

    for iso_node in isolated_nodes:
        if iso_node in processed_iso_nodes: continue
        
        best_partner = None
        best_score = -1.0
        
        # ì „ëµ 1: Zone B (ì ì¬ì  ì—°ê²°) í™œìš©
        for (p1, p2), score in potential_bridges.items():
            partner = None
            if p1 == iso_node: partner = p2
            elif p2 == iso_node: partner = p1
            
            if partner and score > best_score:
                best_score = score
                best_partner = partner

        # ì „ëµ 2: Fallback (S-BERT ìœ ì‚¬ë„ ì „ì²´ ê²€ìƒ‰)
        if not best_partner:
            try: iso_idx = concepts.index(iso_node)
            except: continue
            
            best_sim = -1.0
            for k, other in enumerate(concepts):
                if iso_node == other: continue
                if concept_vectors[iso_idx] is None: continue
                sim = float(cosine_similarity([concept_vectors[iso_idx]], [concept_vectors[k]])[0][0])
                if sim > best_sim:
                    best_sim = sim
                    best_partner = other

        # í›„ë³´ ë“±ë¡
        if best_partner:
            bridge_candidates.append({
                "id": bridge_idx,
                "iso_node": iso_node,
                "partner_node": best_partner
            })
            bridge_idx += 1
            processed_iso_nodes.add(iso_node)


    # ----------------------------------------------------------------
    # [ë°°ì¹˜ ì²˜ë¦¬ 1] Zone C ì°½ì˜ì„± ê²€ì¦ (LLM 1íšŒ í˜¸ì¶œ)
    # ----------------------------------------------------------------
    creative_feedbacks = []
    
    if zone_c_candidates:
        print(f"   [Neuron Map] Zone C ê²€ì¦ {len(zone_c_candidates)}ê±´ ì¼ê´„ ì²˜ë¦¬ ì¤‘...")
        
        # 1. í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ ë¸”ë¡ ìƒì„±
        items_block = ""
        for item in zone_c_candidates:
            items_block += f"- ID {item['id']}: '{item['source']}' - '{item['target']}' (ë¬¸ë§¥: \"{item['context']}\")\n"
        
        # 2. LLM í˜¸ì¶œ
        prompt = CREATIVE_CONNECTION_BATCH_PROMPT.format(items_block=items_block)
        batch_result = _call_llm_json(prompt)
        
        # 3. ê²°ê³¼ ë§¤í•‘
        if batch_result and isinstance(batch_result, list):
            result_map = {res.get('id'): res for res in batch_result}
            
            for item in zone_c_candidates:
                res = result_map.get(item['id'])
                if res:
                    creative_feedbacks.append({
                        "concepts": [item['source'], item['target']],
                        "judgment": res.get('judgment', 'Forced'),
                        "reason": res.get('reason', ''),
                        "feedback": res.get('feedback', '')
                    })
    else:
        print("   [Neuron Map] Zone C(ì°½ì˜ì„± ê²€ì¦) ëŒ€ìƒ ì—†ìŒ.")


    # ----------------------------------------------------------------
    # [ë°°ì¹˜ ì²˜ë¦¬ 2] Bridge ì œì•ˆ ìƒì„± (LLM 1íšŒ í˜¸ì¶œ)
    # ----------------------------------------------------------------
    suggestions = []
    
    if bridge_candidates:
        print(f"   [Neuron Map] Bridge ì œì•ˆ {len(bridge_candidates)}ê±´ ì¼ê´„ ì²˜ë¦¬ ì¤‘...")
        
        # 1. í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ ë¸”ë¡ ìƒì„±
        pairs_block = ""
        for item in bridge_candidates:
            pairs_block += f"- ID {item['id']}: '{item['iso_node']}' <-> '{item['partner_node']}'\n"
            
        # 2. LLM í˜¸ì¶œ
        prompt = BRIDGE_CONCEPT_BATCH_PROMPT.format(
            core_thesis=core_thesis, 
            pairs_block=pairs_block
        )
        batch_result = _call_llm_json(prompt)
        
        # 3. ê²°ê³¼ ë§¤í•‘
        if batch_result and isinstance(batch_result, list):
            result_map = {res.get('id'): res.get('socratic_guide') for res in batch_result}
            
            for item in bridge_candidates:
                guide = result_map.get(item['id'])
                if guide:
                    suggestions.append({
                        "target_node": item['iso_node'],
                        "partner_node": item['partner_node'],
                        "suggestion": guide
                    })
    else:
        print("   [Neuron Map] ì™¸ë”´ ì„¬(Isolated Node) ì—†ìŒ.")


    # ìµœì¢… ì™„ë£Œ
    total_time = time() - start_time
    print(f"âœ… [Neuron Map] ì™„ë£Œ. (ì´ ì†Œìš”ì‹œê°„: {total_time:.3f}ì´ˆ)")
    
    return {
        "nodes": nodes, 
        "edges": edges, 
        "suggestions": suggestions, 
        "creative_feedbacks": creative_feedbacks
    }

def scan_logical_integrity(text):
    """[ê¸°ëŠ¥ 2] ë…¼ë¦¬ ì •í•©ì„± ìŠ¤ìºë„ˆ (Naver)"""
    start_time = time()
    print("ğŸ” [Integrity] (Naver) ì‹œì‘.")
    prompt = INTEGRITY_SCANNER_PROMPT.format(text=text[:4000]) # ë„¤ì´ë²„ í† í° ì œí•œ ê³ ë ¤
    issues = _call_llm_json(prompt)
    print(f"âœ… [Integrity] (Naver) ì™„ë£Œ. ì‹œê°„: {time() - start_time:.3f}ì´ˆ")
    return issues or []

def check_flow_disconnects_with_llm(flow_pattern_json, raw_text):
    """[ê¸°ëŠ¥ 3] íë¦„ ë‹¨ì ˆ ê²€ì‚¬ (ìµœì í™” + ê°€ë…ì„± í–¥ìƒ ì ìš©)"""
    start_time = time()
    print("ğŸŒŠ [Disconnect] (Naver) ì‹œì‘.")
    
    if not flow_pattern_json or 'nodes' not in flow_pattern_json or 'edges' not in flow_pattern_json:
        return []

    nodes = flow_pattern_json['nodes']
    edges = flow_pattern_json['edges']
    
    # 1. ë¬¸ì¥ ë¶„ë¦¬
    split_start = time()
    raw_sentences = [s.strip() for s in re.split(r'[.?!]\s+', raw_text) if len(s.strip()) > 10]
    print(f"   [Debug] ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ ({len(raw_sentences)}ë¬¸ì¥). ì†Œìš”: {time() - split_start:.3f}ì´ˆ")
    
    edges_context = []
    snippets_context = {}

    # ------------------------------------------------------------------
    # [ìµœì í™”] ë³¸ë¬¸ ì„ë² ë”© Pre-calculation
    # ------------------------------------------------------------------
    embed_start = time()
    if embedding_model and raw_sentences:
        doc_embeddings = embedding_model.encode(raw_sentences)
        print(f"   [Debug] ë³¸ë¬¸ ì „ì²´ ì„ë² ë”© ì™„ë£Œ. ì†Œìš”: {time() - embed_start:.3f}ì´ˆ")
    else:
        doc_embeddings = None
        print("   [Debug] ì„ë² ë”© ëª¨ë¸ ì—†ìŒ. ìŠ¤í‚µ.")

    # 2. ì¦ê±° ë¬¸ì¥ ì¶”ì¶œ (Retrieval)
    retrieval_start = time()
    
    for idx, edge in enumerate(edges):
        parent_id, child_id = edge
        
        # ë…¸ë“œ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        parent_full_text = nodes.get(parent_id, "")
        child_full_text = nodes.get(child_id, "")
        
        # [ìˆ˜ì •] ëŒ€ê´„í˜¸ [] ì•ˆì˜ ì¹´í…Œê³ ë¦¬ ëª… ì¶”ì¶œ (ì˜ˆ: "ë¬¸ì œ ì œê¸°", "í•µì‹¬ ì£¼ì¥")
        # ì •ê·œì‹: ë¬¸ìì—´ ì‹œì‘ ë¶€ë¶„ì˜ [ ... ] íŒ¨í„´ì„ ì°¾ìŒ
        p_match = re.search(r'\[(.*?)\]', parent_full_text)
        c_match = re.search(r'\[(.*?)\]', child_full_text)
        
        p_label = p_match.group(1) if p_match else parent_id
        c_label = c_match.group(1) if c_match else child_id
        
        # ìš”ì•½ ë‚´ìš© ì¶”ì¶œ (ë§ˆì§€ë§‰ ì¤„)
        parent_summary = parent_full_text.split('\n')[-1].strip()
        child_summary = child_full_text.split('\n')[-1].strip()
        
        if not parent_summary or not child_summary: continue

        # [ìµœì í™”ëœ ì¶”ì¶œ ë¡œì§]
        p_rep = ""
        c_rep = ""
        
        if embedding_model and doc_embeddings is not None:
            # Parent ì¿¼ë¦¬
            p_query_vec = embedding_model.encode(parent_summary)
            p_sims = cosine_similarity([p_query_vec], doc_embeddings)[0]
            p_idx = np.argmax(p_sims)
            p_rep = raw_sentences[p_idx]

            # Child ì¿¼ë¦¬
            c_query_vec = embedding_model.encode(child_summary)
            c_sims = cosine_similarity([c_query_vec], doc_embeddings)[0]
            c_idx = np.argmax(c_sims)
            c_rep = raw_sentences[c_idx]
        
        # [í•µì‹¬ ìˆ˜ì •] Edge Keyë¥¼ ê°€ë…ì„± ìˆê²Œ ë³€ê²½
        # ì˜ˆ: "[ë¬¸ì œ ì œê¸°] P1 -> [í•µì‹¬ ì£¼ì¥] T1"
        edge_key = f"[{p_label}] {parent_id} -> [{c_label}] {child_id}"
        
        edges_context.append(edge_key)
        snippets_context[edge_key] = {
            "parent_summary": parent_summary,
            "child_summary": child_summary,
            "parent_snippet": p_rep,
            "child_snippet": c_rep
        }

    print(f"   [Debug] ìŠ¤ë‹ˆí« ì¶”ì¶œ ì™„ë£Œ. ì—£ì§€ {len(edges)}ê°œ ì²˜ë¦¬ ì†Œìš”: {time() - retrieval_start:.3f}ì´ˆ")

    if not edges_context: return []

    # 3. LLM íŒê²° (Judge)
    prompt_content = f"""
    {LOGIC_FLOW_CHECK_PROMPT}
    [Structure Edges] {json.dumps(edges_context, ensure_ascii=False)}
    [Text Snippets] {json.dumps(snippets_context, ensure_ascii=False)}
    """

    llm_start = time()
    print(f"   [Debug] LLM í˜¸ì¶œ ì‹œì‘... (ë°ì´í„° í¬ê¸°: {len(prompt_content)} chars)")
    
    # ë„¤ì´ë²„ API í˜¸ì¶œ
    weak_links_result = _call_llm_json(prompt_content)
    
    print(f"   [Debug] LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ. ì†Œìš”: {time() - llm_start:.3f}ì´ˆ")

    # í•„í„°ë§ (Strong ì œì™¸)
    filtered_result = []
    if weak_links_result:
        filtered_result = [
            item for item in weak_links_result 
            if item.get('issue_type') in ['Weak', 'Bridge Needed'] 
        ]

    print(f"âœ… [Disconnect] (Naver) ìµœì¢… ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {time() - start_time:.3f}ì´ˆ")
    return filtered_result
# --------------------------------------------------------------------------------------
# --- 4. ë©”ì¸ ì§„ì… ---
# --------------------------------------------------------------------------------------
def perform_deep_analysis_async(summary_json, raw_text, on_task_complete):
    """
    [ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬]
    3ê°œì˜ ë¶„ì„ ì‘ì—…ì„ ë™ì‹œì— ì‹œì‘í•˜ê³ , ëë‚˜ëŠ” ëŒ€ë¡œ on_task_complete ì½œë°±ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    start_time = time()
    print("\n--- ğŸ§  [DEEP ANALYSIS] ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ---")
    
    key_concepts = summary_json.get('key_concepts', '')
    core_thesis = summary_json.get('Core_Thesis', '')
    flow_pattern = summary_json.get('Flow_Pattern', {})

    # ì‘ì—… ì •ì˜ (í•¨ìˆ˜ëª…, ì¸ì ë¦¬ìŠ¤íŠ¸, ê²°ê³¼ í‚¤ ì´ë¦„)
    tasks = [
        {
            "func": analyze_logic_neuron_map,
            "args": (raw_text, key_concepts, core_thesis),
            "key": "neuron_map"
        },
        {
            "func": scan_logical_integrity,
            "args": (raw_text,),
            "key": "integrity_issues"
        },
        {
            "func": check_flow_disconnects_with_llm,
            "args": (flow_pattern, raw_text),
            "key": "flow_disconnects"
        }
    ]

    results = {}
    
    # ThreadPoolë¡œ 3ê°œ í•¨ìˆ˜ ë™ì‹œ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=3) as executor:
        future_to_key = {
            executor.submit(task["func"], *task["args"]): task["key"] 
            for task in tasks
        }

        for future in as_completed(future_to_key):
            key = future_to_key[future]
            try:
                data = future.result()
                results[key] = data
                print(f"âš¡ [Async] '{key}' ì™„ë£Œ. DB ì—…ë°ì´íŠ¸ ìš”ì²­.")
                
                # [í•µì‹¬] ì‘ì—… í•˜ë‚˜ ëë‚  ë•Œë§ˆë‹¤ ì½œë°± í˜¸ì¶œ -> DB ì €ì¥
                if on_task_complete:
                    on_task_complete(key, data)
                    
            except Exception as e:
                print(f"âŒ [Async Error] '{key}' ì‹¤íŒ¨: {e}")
                if on_task_complete:
                    on_task_complete(key, {"error": str(e)})

    total_time = time() - start_time
    print(f"--- âœ… [DEEP ANALYSIS] ì „ì²´ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ. ì‹œê°„: {total_time:.3f}ì´ˆ ---\n")
    return results