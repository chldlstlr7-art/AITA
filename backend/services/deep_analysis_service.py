import os
import json
import re
import requests # ğŸ“¦ ë„¤ì´ë²„ API í˜¸ì¶œì„ ìœ„í•´ ì¶”ê°€
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from time import time, sleep
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
# í”„ë¡¬í”„íŠ¸ ì„¤ì • ë¡œë“œ
from config import INTEGRITY_SCANNER_PROMPT, BRIDGE_CONCEPT_PROMPT, LOGIC_FLOW_CHECK_PROMPT, CREATIVE_CONNECTION_PROMPT

# --------------------------------------------------------------------------------------
# --- 1. ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ ---
# --------------------------------------------------------------------------------------

# [ë„¤ì´ë²„ API ì„¤ì •]
# [ì„¤ì • ë³€ê²½]
# NAVER_GATEWAY_KEYëŠ” ì´ì œ ì‚­ì œí•˜ì…”ë„ ë©ë‹ˆë‹¤.
NAVER_CLOVA_URL = os.environ.get('NAVER_CLOVA_URL2') # "https://clovastudio.stream..."
NAVER_API_KEY = os.environ.get('NAVER_API_KEY')     # "nv-...." (ìƒˆë¡œ ë°œê¸‰ë°›ì€ í‚¤)

# S-BERT ì„¤ì • (ìœ ì§€)
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
embedding_model = None

try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print(f"[Service Deep Analysis] Embedding Model loaded.")
except Exception as e:
    print(f"[Service Deep Analysis] CRITICAL: Embedding Model Failed: {e}")

# --------------------------------------------------------------------------------------
# --- 2. í—¬í¼ í•¨ìˆ˜ (Naver HyperCLOVA X í˜¸ì¶œ) ---
# --------------------------------------------------------------------------------------
def _call_llm_json(prompt_text):
    """
    [ìµœì‹ ] Naver HyperCLOVA X API í˜¸ì¶œ (Bearer Token ë°©ì‹)
    """
    if not NAVER_CLOVA_URL or not NAVER_API_KEY:
        print("âš ï¸ Naver API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # [í•µì‹¬ ë³€ê²½] í—¤ë”ê°€ ì•„ì£¼ ì‹¬í”Œí•´ì¡ŒìŠµë‹ˆë‹¤.
    headers = {
    'Authorization': f'Bearer {NAVER_API_KEY}',
    'Content-Type': 'application/json; charset=utf-8',
    'Accept': 'application/json'   # <--- ìˆ˜ì •! (ì™„ì„±ëœ JSONìœ¼ë¡œ ë‹¬ë¼ëŠ” ëœ»)
    }

    # HyperCLOVA X ìš”ì²­ íŒŒë¼ë¯¸í„° (ë©”ì‹œì§€ êµ¬ì¡°ëŠ” ë™ì¼)
    data = {
        "messages": [
            {
                "role": "system",
                "content": "ë„ˆëŠ” ë…¼ë¦¬ì ì¸ í•™ìˆ  ë©˜í† ì•¼. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í¬ë§·ìœ¼ë¡œë§Œ ì¶œë ¥í•´. ë§ˆí¬ë‹¤ìš´ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¤˜."
            },
            {
                "role": "user",
                "content": prompt_text
            }
        ],
        "topP": 0.8,
        "topK": 0,
        "maxCompletionTokens": 4096,
        "temperature": 0.2,
        "repeatPenalty": 1.5,
        "stopBefore": [],
        "includeAiFilters": True,
        "seed": 0
    }

    
    try:
        response = requests.post(NAVER_CLOVA_URL, headers=headers, json=data, stream=False)
        response.raise_for_status()
        
        res_json = response.json()
        content_text = res_json.get('result', {}).get('message', {}).get('content', '')
        
        if not content_text:
            print(f"[Naver] Empty content received.")
            return None

        # --- [JSON ì¶”ì¶œ ë° íŒŒì‹± ë¡œì§ ê°•í™”] ---
        
        # 1. Markdown ì½”ë“œë¸”ë¡ ì œê±°
        json_str = ""
        match = re.search(r"```json\s*([\s\S]+?)\s*```", content_text)
        if match:
            json_str = match.group(1)
        else:
            # ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ì¶”ì¶œ ì‹œë„
            json_match = re.search(r"(\{[\s\S]*\}|\[[\s\S]*\])", content_text.strip())
            json_str = json_match.group(1) if json_match else content_text.strip()

        # 2. íŒŒì‹± ì‹œë„ (3ë‹¨ê³„ ë°©ì–´ ì „ëµ)
        
        # [1ì°¨ ì‹œë„] í‘œì¤€ json.loads (strict=False)
        try:
            return json.loads(json_str, strict=False)
        except json.JSONDecodeError:
            pass # ì‹¤íŒ¨ ì‹œ 2ì°¨ ì‹œë„ë¡œ ë„˜ì–´ê°

        # [2ì°¨ ì‹œë„] ast.literal_eval (Python êµ¬ì¡° íŒŒì‹±)
        # LLMì´ ê°€ë” JSON ëŒ€ì‹  Python Dict í˜•íƒœ(True/False, ì‹±ê¸€ì¿¼íŠ¸ ë“±)ë¥¼ ì¤„ ë•Œ ìœ ìš©í•¨
        try:
            return ast.literal_eval(json_str)
        except:
            pass

        # [3ì°¨ ì‹œë„] í”í•œ ì˜¤ë¥˜(ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ìŒë”°ì˜´í‘œ) ìˆ˜ë™ ìˆ˜ì • í›„ ì¬ì‹œë„
        try:
            # "quote": "..." íŒ¨í„´ ì•ˆì˜ ë‚´ìš©ë¬¼ì€ ê±´ë“œë¦¬ì§€ ì•Šê³ , êµ¬ì¡°ë¥¼ ë§ê°€ëœ¨ë¦¬ëŠ” ê²ƒë§Œ ìˆ˜ì •í•˜ê¸´ ì–´ë µì§€ë§Œ
            # ë‹¨ìˆœí•˜ê²Œ ì¤„ë°”ê¿ˆ ë¬¸ì œì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
            json_str_clean = json_str.replace('\n', '\\n').replace('\r', '')
            return json.loads(json_str_clean, strict=False)
        except:
            pass
            
        print(f"[JSON Parsing Failed] Content: {content_text[:200]}...")
        return None

    except Exception as e:
        print(f"[Naver API Error] {e}")
        return None

# --- (ì•„ë˜ S-BERT ê´€ë ¨ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€) ---

def _get_embedding(text):
    if not embedding_model: return None
    return embedding_model.encode(text)

def _calculate_similarity(text_a, text_b):
    vec_a = _get_embedding(text_a)
    vec_b = _get_embedding(text_b)
    if vec_a is None or vec_b is None: return 0.0
    return float(cosine_similarity([vec_a], [vec_b])[0][0])

def extract_representative_sentences(text_sentences, query_summary, top_k=1):
    if not text_sentences or not embedding_model: return ""
    sentence_embeddings = embedding_model.encode(text_sentences)
    query_embedding = embedding_model.encode(query_summary)
    similarities = cosine_similarity([query_embedding], sentence_embeddings)[0]
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return text_sentences[top_indices[0]] if len(top_indices) > 0 else ""

# --------------------------------------------------------------------------------------
# --- 3. í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ (ë¡œì§ì€ ìœ ì§€í•˜ë˜, ìˆœì°¨ ì²˜ë¦¬ëŠ” API ì œí•œì— ë”°ë¼ ì¡°ì •) ---
# --------------------------------------------------------------------------------------
# ë„¤ì´ë²„ ìœ ë£Œ APIëŠ” ë³´í†µ Rate Limitì´ ë„‰ë„‰í•˜ë¯€ë¡œ ë‹¤ì‹œ 'ë³‘ë ¬ ì²˜ë¦¬'ë¥¼ ì‹œë„í•´ë³¼ ë§Œí•©ë‹ˆë‹¤.
# í•˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ 'ìˆœì°¨ ì²˜ë¦¬' ì½”ë“œë¥¼ ìœ ì§€í•˜ê² ìŠµë‹ˆë‹¤.

def analyze_logic_neuron_map(text, key_concepts_str, core_thesis):
    """
    [Zone ê¸°ë°˜ ê³ ë„í™”] ë…¼ë¦¬ ë‰´ëŸ° ë§µ ìƒì„±
    - Zone A (Strong): ì—£ì§€ ìƒì„± (ì‹¤ì„ )
    - Zone B (Bridge): ì™¸ë”´ ì„¬ ë°œìƒ ì‹œ ìš°ì„  ì—°ê²° í›„ë³´ë¡œ ì‚¬ìš©
    - Zone C (Creative): ì—£ì§€ ìƒì„± (ë¬¼ê²°ì„ ) + LLM ì°½ì˜ì„± ê²€ì¦ ìˆ˜í–‰
    """
    start_time = time()
    print("ğŸš€ [Neuron Map] (Naver) 1/4 ì‹œì‘: Zone ê¸°ë°˜ ë¶„ì„.")
    
    if not key_concepts_str: return {"nodes": [], "edges": [], "suggestions": [], "creative_feedbacks": []}
    
    concepts = [c.strip() for c in key_concepts_str.split(',') if c.strip()]
    if not concepts: return {"nodes": [], "edges": [], "suggestions": [], "creative_feedbacks": []}

    nodes = [{"id": c, "label": c} for c in concepts]
    edges = []
    
    # ê° ë…¸ë“œì˜ ì—°ê²° ìƒíƒœ ì¶”ì  (Trueë©´ ì™¸ë”´ ì„¬ ì•„ë‹˜)
    connected_status = {c: False for c in concepts}
    
    # Zone B (ì ì¬ì  ì—°ê²°) í›„ë³´ ì €ì¥ì†Œ: {(c1, c2): semantic_score}
    potential_bridges = {}
    
    # Zone C (ì°½ì˜ì„± ê²€ì¦) Task ì €ì¥ì†Œ
    zone_c_tasks = []

    paragraphs = [p for p in text.split('\n') if len(p) > 20]

    # 1. S-BERT Batch Encoding
    if embedding_model:
        concept_vectors = embedding_model.encode(concepts) 
    else:
        concept_vectors = [None] * len(concepts) 

    # 2. Pairwise ë¶„ì„ (N x N)
    for i in range(len(concepts)):
        for j in range(i + 1, len(concepts)):
            c1 = concepts[i]
            c2 = concepts[j]
            
            # (A) ë¬¼ë¦¬ì  ê±°ë¦¬ (0.0 ~ 1.0)
            physical_score = 0.0
            context_sent = "" # Zone C ê²€ì¦ìš© ë¬¸ì¥
            for p in paragraphs:
                if c1 in p and c2 in p:
                    physical_score += 1.0
                    if not context_sent: context_sent = p # ì²« ë²ˆì§¸ ë°œê²¬ëœ ë¬¸ì¥ ì €ì¥
            physical_score = min(physical_score / 2.0, 1.0) # 2ë²ˆë§Œ ê°™ì´ ë‚˜ì™€ë„ ë§Œì  (ì™„í™”)

            # (B) ì˜ë¯¸ì  ê±°ë¦¬ (0.0 ~ 1.0)
            if concept_vectors[i] is not None:
                semantic_score = float(cosine_similarity([concept_vectors[i]], [concept_vectors[j]])[0][0])
            else:
                semantic_score = 0.0
            
            # --- ğŸ“Š Zone íŒë³„ ë¡œì§ ---
            
            # 1. Zone C: ì°½ì˜ì /ì‘ìœ„ì  ì—°ê²° (ì˜ë¯¸ ë©‚ + ë¬¼ë¦¬ ê°€ê¹Œì›€)
            # S-BERTëŠ” ë©€ë‹¤ê³  í•˜ëŠ”ë°(0.4 ë¯¸ë§Œ), ê¸€ì—ì„œëŠ” ë¶™ì—¬ë†“ìŒ(0.5 ì´ìƒ)
            if semantic_score < 0.4 and physical_score >= 0.5:
                edges.append({
                    "source": c1, "target": c2, 
                    "weight": round(semantic_score, 2),
                    "type": "questionable" # í”„ë¡ íŠ¸ì—ì„œ ë¬¼ê²°ì„ /ì ì„  ë“±ìœ¼ë¡œ í‘œì‹œ
                })
                connected_status[c1] = True
                connected_status[c2] = True
                
                # LLM ê²€ì¦ ëŒ€ê¸°ì—´ ì¶”ê°€
                if context_sent:
                    prompt = CREATIVE_CONNECTION_PROMPT.format(
                        concept_a=c1, concept_b=c2, context_sentence=context_sent
                    )
                    zone_c_tasks.append({"source": c1, "target": c2, "prompt": prompt})

            # 2. Zone B: ì ì¬ì  ì—°ê²° (ì˜ë¯¸ ê°€ê¹Œì›€ + ë¬¼ë¦¬ ë©‚)
            # S-BERTëŠ” ê°€ê¹ë‹¤ê³  í•˜ëŠ”ë°(0.65 ì´ìƒ), ê¸€ì—ì„œëŠ” ë”°ë¡œ ë†ˆ(0.2 ë¯¸ë§Œ)
            elif semantic_score > 0.65 and physical_score < 0.2:
                # ì—£ì§€ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ê¸€ì— ì—†ìœ¼ë‹ˆê¹Œ)
                # ë‚˜ì¤‘ì— ì™¸ë”´ ì„¬ ë°œìƒ ì‹œ, ì´ ì»¤í”Œì„ ìµœìš°ì„ ìœ¼ë¡œ ì¶”ì²œí•¨
                potential_bridges[(c1, c2)] = semantic_score
                # (ì£¼ì˜: connected_statusëŠ” Trueë¡œ ë°”ê¾¸ì§€ ì•ŠìŒ -> ì™¸ë”´ ì„¬ìœ¼ë¡œ ë‚¨ê²¨ë‘ )

            # 3. Zone A & Normal: ì¼ë°˜ì ì¸ ì—°ê²° (ê°€ì¤‘ì¹˜ í•©ì‚°)
            else:
                total_weight = (physical_score * 0.4) + (semantic_score * 0.6)
                if total_weight > 0.35:
                    edges.append({
                        "source": c1, "target": c2, 
                        "weight": round(total_weight, 2),
                        "type": "strong" if total_weight > 0.65 else "normal"
                    })
                    connected_status[c1] = True
                    connected_status[c2] = True

    # 3. ì™¸ë”´ ì„¬(Isolated Node) êµ¬ì¶œ ì‘ì „ (Bridge ì œì•ˆ)
    suggestions = []
    bridge_tasks = []
    
    # ì•„ì§ ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œë“¤ ì°¾ê¸°
    isolated_nodes = [node for node, connected in connected_status.items() if not connected]
    
    processed_iso_nodes = set() # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€

    for iso_node in isolated_nodes:
        if iso_node in processed_iso_nodes: continue
        
        best_partner = None
        
        # ì „ëµ 1: Zone B (ì ì¬ì  ì—°ê²°) ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒŒíŠ¸ë„ˆê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
        # (ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ë†ˆì„ ì°¾ìŒ)
        best_zone_b_score = -1.0
        
        for (p1, p2), score in potential_bridges.items():
            partner = None
            if p1 == iso_node: partner = p2
            elif p2 == iso_node: partner = p1
            
            if partner and score > best_zone_b_score:
                best_zone_b_score = score
                best_partner = partner

        # ì „ëµ 2: Zone Bì—ë„ ì—†ë‹¤ë©´, ê·¸ëƒ¥ ì „ì²´ ì¤‘ì—ì„œ S-BERT ê°€ì¥ ë†’ì€ ë†ˆ ì°¾ê¸° (Fallback)
        if not best_partner:
            try: iso_idx = concepts.index(iso_node)
            except: continue
            
            best_sim = -1.0
            for k, other in enumerate(concepts):
                if iso_node == other: continue
                if concept_vectors[iso_idx] is None: continue
                sim = float(cosine_similarity([concept_vectors[iso_idx]], [concept_vectors[k]])[0][0])
                if sim > best_sim:
                    best_sim = sim
                    best_partner = other

        # Task ì¶”ê°€
        if best_partner:
            prompt = BRIDGE_CONCEPT_PROMPT.format(
                concept_a=iso_node, concept_b=best_partner, core_thesis=core_thesis
            )
            bridge_tasks.append({
                "iso_node": iso_node, "partner": best_partner, "prompt": prompt
            })
            processed_iso_nodes.add(iso_node)

    # 4. LLM ìˆœì°¨ í˜¸ì¶œ (Bridge ì œì•ˆ)
    if bridge_tasks:
        print(f"   [Neuron Map] 3/4 Bridge ì œì•ˆ {len(bridge_tasks)}ê±´ ìˆœì°¨ ì²˜ë¦¬.")
        for i, task in enumerate(bridge_tasks):
            if i > 0: sleep(1.0) # Rate Limit ë°©ì§€
            res = _call_llm_json(task['prompt'])
            if res:
                suggestions.append({
                    "target_node": task['iso_node'],
                    "partner_node": task['partner'],
                    "suggestion": res
                })

    # 5. LLM ìˆœì°¨ í˜¸ì¶œ (Zone C ì°½ì˜ì„± ê²€ì¦)
    creative_feedbacks = []
    if zone_c_tasks:
        print(f"   [Neuron Map] 4/4 Zone C(ì°½ì˜ì„±) ê²€ì¦ {len(zone_c_tasks)}ê±´ ìˆœì°¨ ì²˜ë¦¬.")
        for i, task in enumerate(zone_c_tasks):
            if i > 0 or bridge_tasks: sleep(1.0) # ì• ì‘ì—…ì´ ìˆì—ˆìœ¼ë©´ íœ´ì‹
            res = _call_llm_json(task['prompt'])
            if res:
                creative_feedbacks.append({
                    "concepts": [task['source'], task['target']],
                    "judgment": res.get('judgment'),
                    "reason": res.get('reason'),
                    "feedback": res.get('feedback')
                })
    
    total_time = time() - start_time
    print(f"âœ… [Neuron Map] (Naver) ì™„ë£Œ. ì‹œê°„: {total_time:.3f}ì´ˆ")
    
    return {
        "nodes": nodes, 
        "edges": edges, 
        "suggestions": suggestions,         # Zone B ê¸°ë°˜ (ì™¸ë”´ ì„¬ ì—°ê²°)
        "creative_feedbacks": creative_feedbacks # Zone C ê¸°ë°˜ (ì°½ì˜/ì–µì§€ íŒë‹¨)
    }

def scan_logical_integrity(text):
    """[ê¸°ëŠ¥ 2] ë…¼ë¦¬ ì •í•©ì„± ìŠ¤ìºë„ˆ (Naver)"""
    start_time = time()
    print("ğŸ” [Integrity] (Naver) ì‹œì‘.")
    prompt = INTEGRITY_SCANNER_PROMPT.format(text=text[:4000]) # ë„¤ì´ë²„ í† í° ì œí•œ ê³ ë ¤
    issues = _call_llm_json(prompt)
    print(f"âœ… [Integrity] (Naver) ì™„ë£Œ. ì‹œê°„: {time() - start_time:.3f}ì´ˆ")
    return issues or []

def check_flow_disconnects_with_llm(flow_pattern_json, raw_text):
    """[ê¸°ëŠ¥ 3] íë¦„ ë‹¨ì ˆ ê²€ì‚¬ (ìµœì í™” + ë””ë²„ê¹… ì ìš©)"""
    start_time = time()
    print("ğŸŒŠ [Disconnect] (Naver) ì‹œì‘.")
    
    if not flow_pattern_json or 'nodes' not in flow_pattern_json or 'edges' not in flow_pattern_json:
        return []

    nodes = flow_pattern_json['nodes']
    edges = flow_pattern_json['edges']
    
    # 1. ë¬¸ì¥ ë¶„ë¦¬
    split_start = time()
    raw_sentences = [s.strip() for s in re.split(r'[.?!]\s+', raw_text) if len(s.strip()) > 10]
    print(f"   [Debug] ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ ({len(raw_sentences)}ë¬¸ì¥). ì†Œìš”: {time() - split_start:.3f}ì´ˆ")
    
    edges_context = []
    snippets_context = {}

    # ------------------------------------------------------------------
    # [ìµœì í™” í•µì‹¬] ë³¸ë¬¸ ì„ë² ë”©ì„ ë£¨í”„ ë°–ì—ì„œ 1íšŒë§Œ ìˆ˜í–‰ (Pre-calculation)
    # ------------------------------------------------------------------
    embed_start = time()
    if embedding_model and raw_sentences:
        # ë³¸ë¬¸ ì „ì²´ë¥¼ í•œ ë²ˆì— ë²¡í„°í™” (ê°€ì¥ ë¬´ê±°ìš´ ì‘ì—…)
        doc_embeddings = embedding_model.encode(raw_sentences)
        print(f"   [Debug] ë³¸ë¬¸ ì „ì²´ ì„ë² ë”© ì™„ë£Œ. ì†Œìš”: {time() - embed_start:.3f}ì´ˆ")
    else:
        doc_embeddings = None
        print("   [Debug] ì„ë² ë”© ëª¨ë¸ ì—†ìŒ. ìŠ¤í‚µ.")

    # 2. ì¦ê±° ë¬¸ì¥ ì¶”ì¶œ (Retrieval)
    retrieval_start = time()
    
    for idx, edge in enumerate(edges):
        parent_id, child_id = edge
        parent_summary = nodes.get(parent_id, "").split('\n')[-1].strip()
        child_summary = nodes.get(child_id, "").split('\n')[-1].strip()
        
        if not parent_summary or not child_summary: continue

        # [ìµœì í™”ëœ ì¶”ì¶œ ë¡œì§]
        # ì´ë¯¸ ê³„ì‚°ëœ doc_embeddingsë¥¼ ì¬ì‚¬ìš©í•˜ë¯€ë¡œ ì†ë„ê°€ ë§¤ìš° ë¹ ë¦„ (ë‹¨ìˆœ í–‰ë ¬ê³± ì—°ì‚°)
        p_rep = ""
        c_rep = ""
        
        if embedding_model and doc_embeddings is not None:
            # Parent ì¿¼ë¦¬ ì„ë² ë”©
            p_query_vec = embedding_model.encode(parent_summary)
            p_sims = cosine_similarity([p_query_vec], doc_embeddings)[0]
            p_idx = np.argmax(p_sims) # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ ì¸ë±ìŠ¤
            p_rep = raw_sentences[p_idx]

            # Child ì¿¼ë¦¬ ì„ë² ë”©
            c_query_vec = embedding_model.encode(child_summary)
            c_sims = cosine_similarity([c_query_vec], doc_embeddings)[0]
            c_idx = np.argmax(c_sims)
            c_rep = raw_sentences[c_idx]
        
        edge_key = f"{parent_id}->{child_id}"
        edges_context.append(edge_key)
        snippets_context[edge_key] = {
            "parent_summary": parent_summary,
            "child_summary": child_summary,
            "parent_snippet": p_rep,
            "child_snippet": c_rep
        }

    print(f"   [Debug] ìŠ¤ë‹ˆí« ì¶”ì¶œ(Retrieval) ì™„ë£Œ. ì—£ì§€ {len(edges)}ê°œ ì²˜ë¦¬ ì†Œìš”: {time() - retrieval_start:.3f}ì´ˆ")

    if not edges_context: return []

    # 3. LLM íŒê²° (Judge)
    prompt_content = f"""
    {LOGIC_FLOW_CHECK_PROMPT}
    [Structure Edges] {json.dumps(edges_context, ensure_ascii=False)}
    [Text Snippets] {json.dumps(snippets_context, ensure_ascii=False)}
    """

    llm_start = time()
    print(f"   [Debug] LLM í˜¸ì¶œ ì‹œì‘... (ë°ì´í„° í¬ê¸°: {len(prompt_content)} chars)")
    
    # ì—¬ê¸°ì„œ ì‹œê°„ì´ ê°€ì¥ ë§ì´ ê±¸ë¦¼ (ë„¤ì´ë²„ ì„œë²„ ì²˜ë¦¬ ì‹œê°„)
    weak_links_result = _call_llm_json(prompt_content)
    
    print(f"   [Debug] LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ. ì†Œìš”: {time() - llm_start:.3f}ì´ˆ")

    # í•„í„°ë§ (Strong ì œì™¸)
    filtered_result = []
    if weak_links_result:
        filtered_result = [
            item for item in weak_links_result 
            if item.get('issue_type') in ['Weak', 'Bridge Needed'] 
        ]

    print(f"âœ… [Disconnect] (Naver) ìµœì¢… ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {time() - start_time:.3f}ì´ˆ")
    return filtered_result
# --------------------------------------------------------------------------------------
# --- 4. ë©”ì¸ ì§„ì… ---
# --------------------------------------------------------------------------------------
def perform_deep_analysis_async(summary_json, raw_text, on_task_complete):
    """
    [ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬]
    3ê°œì˜ ë¶„ì„ ì‘ì—…ì„ ë™ì‹œì— ì‹œì‘í•˜ê³ , ëë‚˜ëŠ” ëŒ€ë¡œ on_task_complete ì½œë°±ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    start_time = time()
    print("\n--- ğŸ§  [DEEP ANALYSIS] ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ---")
    
    key_concepts = summary_json.get('key_concepts', '')
    core_thesis = summary_json.get('Core_Thesis', '')
    flow_pattern = summary_json.get('Flow_Pattern', {})

    # ì‘ì—… ì •ì˜ (í•¨ìˆ˜ëª…, ì¸ì ë¦¬ìŠ¤íŠ¸, ê²°ê³¼ í‚¤ ì´ë¦„)
    tasks = [
        {
            "func": analyze_logic_neuron_map,
            "args": (raw_text, key_concepts, core_thesis),
            "key": "neuron_map"
        },
        {
            "func": scan_logical_integrity,
            "args": (raw_text,),
            "key": "integrity_issues"
        },
        {
            "func": check_flow_disconnects_with_llm,
            "args": (flow_pattern, raw_text),
            "key": "flow_disconnects"
        }
    ]

    results = {}
    
    # ThreadPoolë¡œ 3ê°œ í•¨ìˆ˜ ë™ì‹œ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=3) as executor:
        future_to_key = {
            executor.submit(task["func"], *task["args"]): task["key"] 
            for task in tasks
        }

        for future in as_completed(future_to_key):
            key = future_to_key[future]
            try:
                data = future.result()
                results[key] = data
                print(f"âš¡ [Async] '{key}' ì™„ë£Œ. DB ì—…ë°ì´íŠ¸ ìš”ì²­.")
                
                # [í•µì‹¬] ì‘ì—… í•˜ë‚˜ ëë‚  ë•Œë§ˆë‹¤ ì½œë°± í˜¸ì¶œ -> DB ì €ì¥
                if on_task_complete:
                    on_task_complete(key, data)
                    
            except Exception as e:
                print(f"âŒ [Async Error] '{key}' ì‹¤íŒ¨: {e}")
                if on_task_complete:
                    on_task_complete(key, {"error": str(e)})

    total_time = time() - start_time
    print(f"--- âœ… [DEEP ANALYSIS] ì „ì²´ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ. ì‹œê°„: {total_time:.3f}ì´ˆ ---\n")
    return results