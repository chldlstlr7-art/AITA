import os
import json
import re
import numpy as np
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from time import sleep

from extensions import db
from models import AnalysisReport

# --------------------------------------------------------------------------------------
# --- 1. ì „ì—­ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ (Flask ì•± ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰) ---
# --------------------------------------------------------------------------------------

GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
MAX_RETRIES = 3

# [ëª¨ë¸ ì„¤ì •]
# (config.pyì˜ í”„ë¡¬í”„íŠ¸ê°€ Gemini 2.5 Flash/Proì— ìµœì í™”ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
ANALYSIS_MODEL_NAME = 'gemini-2.5-flash' # 1ë‹¨ê³„ ë¶„ì„/ìš”ì•½ìš©
COMPARISON_MODEL_NAME = 'gemini-2.5-flash' # 3ë‹¨ê³„ ë¹„êµìš©
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2' # 2ë‹¨ê³„ S-BERTìš©

# [LLM í´ë¼ì´ì–¸íŠ¸]
llm_client_analysis = None
llm_client_comparison = None

if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        llm_client_analysis = genai.GenerativeModel(ANALYSIS_MODEL_NAME)
        llm_client_comparison = genai.GenerativeModel(COMPARISON_MODEL_NAME)
        print(f"[Service Analysis] LLM Models ({ANALYSIS_MODEL_NAME}, {COMPARISON_MODEL_NAME}) loaded.")
    except Exception as e:
        print(f"[Service Analysis] CRITICAL: Gemini Client failed to load: {e}")
else:
    print("[Service Analysis] WARNING: GEMINI_API_KEY not found. LLM Analysis will fail.")

# [S-BERT ëª¨ë¸]
try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print(f"[Service Analysis] Embedding Model '{EMBEDDING_MODEL_NAME}' loaded.")
except Exception as e:
    print(f"[Service Analysis] CRITICAL: Failed to load SentenceTransformer model: {e}")
    embedding_model = None

# [DB ì„¤ì •]
# (DB ë¡œë“œ ë¡œì§ ì‚­ì œ -> Flaskì˜ db ê°ì²´ ì‚¬ìš©)
print("[Service Analysis] Ready. (DB will be accessed via Flask context)")

# ----------------------------------------------------
# --- 2. í—¬í¼ í•¨ìˆ˜ ì •ì˜ (ë‚´ë¶€ìš©) ---
# ----------------------------------------------------

def _llm_call_analysis(raw_text, system_prompt):
    """(1ë‹¨ê³„ ë¶„ì„ìš©) Gemini ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ JSON êµ¬ì¡°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."""
    if not llm_client_analysis: return None
    
    # [ìˆ˜ì •] ìƒˆ í”„ë¡¬í”„íŠ¸ëŠ” MIME-TYPE ëŒ€ì‹  ```json ... ```ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, 
    # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µì„ ë°›ê³  re.searchë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    config = genai.GenerationConfig(response_mime_type="text/plain") 
    
    prompt_content = f"{system_prompt}\n\nTarget Text: \n\n{raw_text[:10000]}..."
    
    for attempt in range(MAX_RETRIES):
        try:
            response = llm_client_analysis.generate_content(
                contents=[prompt_content], 
                generation_config=config
            )
            if not response.text: raise Exception("Empty response (Analysis)")
            
            # LLM ì‘ë‹µì—ì„œ JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
            match = re.search(r"```json\s*([\s\S]+?)\s*```", response.text)
            if not match:
                print(f"[Service Analysis] LLM_ANALYSIS_FAILED: JSON í˜•ì‹ ì‘ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Raw: {response.text[:200]}...")
                raise Exception("JSON format not found in LLM response.")
            
            return json.loads(match.group(1)) # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        
        except Exception as e:
            print(f"[Service Analysis] LLM Call Error (Attempt {attempt + 1}): {e}")
            if attempt < MAX_RETRIES - 1: sleep(2**attempt)
            else: print(f"[Service Analysis] Final Error (Analysis): {e}")
    return None

def _llm_call_comparison(submission_json_str, candidate_json_str, system_prompt_template):
    """(3ë‹¨ê³„ ë¹„êµìš©) Gemini ëª¨ë¸ë¡œ ë‘ JSONì„ 1:1 ë¹„êµí•©ë‹ˆë‹¤."""
    if not llm_client_comparison: return None
    
    # [ìˆ˜ì •] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— JSON ë¬¸ìì—´ì„ ì£¼ì…í•©ë‹ˆë‹¤.
    user_prompt = system_prompt_template.format(
        submission_json_str=submission_json_str,
        candidate_json_str=candidate_json_str
    )
    
    for attempt in range(MAX_RETRIES):
        try:
            response = llm_client_comparison.generate_content(contents=[user_prompt])
            if not response.text: raise Exception("Empty response (Comparison)")
            return response.text # 6ê°œ í•­ëª© ì ìˆ˜ í…ìŠ¤íŠ¸ ë°˜í™˜
        
        except Exception as e:
            print(f"[Service Analysis] LLM Call Error (Attempt {attempt + 1}): {e}")
            if attempt < MAX_RETRIES - 1: sleep(2**attempt)
            else: print(f"[Service Analysis] Final Error (Comparison): {e}")
    return None

def get_embedding_vector(text):
    """[ì‹ ê·œ] í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë²¡í„°(list)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not embedding_model:
        print("[get_embedding_vector] ERROR: ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    try:
        vector = embedding_model.encode(text)
        return vector.tolist() # DB ì €ì¥ì„ ìœ„í•´ listë¡œ ë³€í™˜
    except Exception as e:
        print(f"[get_embedding_vector] ERROR: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def build_concat_text(key_concepts, main_idea):
    """[ì‹ ê·œ] ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¡°í•© (0.6:0.4 ë¡œì§ ê¸°ë°˜)"""
    return f"ì£¼ìš” ê°œë…: {key_concepts}\ní•µì‹¬ ì•„ì´ë””ì–´: {main_idea}"

def find_similar_documents(submission_id, sub_thesis_vec, sub_claim_vec, top_n=3):
    """
    [Full Code]
    í•­ìƒ ì‹¤ì‹œê°„ SQL DB (AnalysisReport)ë¥¼ ì¿¼ë¦¬í•©ë‹ˆë‹¤.
    is_test=Falseì¸ ë¦¬í¬íŠ¸ë§Œ ë¹„êµ ëŒ€ì¡°êµ°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    # 1. ì œì¶œëœ ë²¡í„°ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
    try:
        sub_thesis_np = np.array(sub_thesis_vec).reshape(1, -1)
        sub_claim_np = np.array(sub_claim_vec).reshape(1, -1)
    except Exception as e:
        print(f"[find_similar_documents] ERROR: ì œì¶œëœ ë²¡í„°ë¥¼ NumPyë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return []
        
    db_ids = []
    db_summaries_json = [] # ë¹„êµìš© ìš”ì•½ë³¸ (JSON ë¬¸ìì—´)
    db_vectors_thesis_list = []
    db_vectors_claim_list = []
    db_original_filenames = []

    db_vectors_thesis_np = None
    db_vectors_claim_np = None

    # --- 2. í•­ìƒ ì‹¤ì‹œê°„ ëª¨ë“œ (SQL DB ì¿¼ë¦¬) ---
    print(f"[find_similar_documents] Using Live DB Query (is_test=False, Excluding ID: {submission_id})")
    try:
        # [CRITICAL] 'is_test=False'ì¸ ë¦¬í¬íŠ¸ë§Œ ë¹„êµ ëŒ€ì¡°êµ°ìœ¼ë¡œ ì‚¬ìš©
        query = AnalysisReport.query.filter(
            AnalysisReport.embedding_keyconcepts_corethesis.isnot(None),
            AnalysisReport.embedding_keyconcepts_claim.isnot(None),
            AnalysisReport.is_test == False
        )
        
        # 'ìê¸° ìì‹  ì œì™¸' ë¡œì§
        if submission_id:
             query = query.filter(AnalysisReport.id != submission_id)
        
        all_reports = query.all()

        if not all_reports:
            print("[find_similar_documents] ë¹„êµí•  DB ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. (is_test=False í•„í„°ë§ë¨)")
            return []

        # 3. DB ê²°ê³¼ì—ì„œ ë²¡í„° ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        for report in all_reports:
            try:
                db_ids.append(report.id)
                db_summaries_json.append(report.summary) # JSON ë¬¸ìì—´
                db_original_filenames.append(report.original_filename)
                vec_thesis = json.loads(report.embedding_keyconcepts_corethesis)
                vec_claim = json.loads(report.embedding_keyconcepts_claim)
                
                db_vectors_thesis_list.append(vec_thesis)
                db_vectors_claim_list.append(vec_claim)
                
            except Exception as e:
                print(f"[find_similar_documents] Report {report.id} ì„ë² ë”©/ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        if not db_ids:
            print("[find_similar_documents] ìœ íš¨í•œ DB ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # 4. DB ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
        db_vectors_thesis_np = np.array(db_vectors_thesis_list)
        db_vectors_claim_np = np.array(db_vectors_claim_list)

    except Exception as e:
        print(f"[find_similar_documents] CRITICAL: Live DB ì¿¼ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
    # --- ì¿¼ë¦¬ ì¢…ë£Œ ---

    if db_vectors_thesis_np is None or db_vectors_claim_np.shape[0] == 0:
        print("[find_similar_documents] DB ë²¡í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []
        
    # 5. ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¤‘í•© 0.6:0.4)
    sim_thesis = cosine_similarity(sub_thesis_np, db_vectors_thesis_np)[0]
    sim_claim = cosine_similarity(sub_claim_np, db_vectors_claim_np)[0]

    WEIGHT_THESIS = 0.6
    WEIGHT_CLAIM = 0.4
    sim_weighted = WEIGHT_THESIS * sim_thesis + WEIGHT_CLAIM * sim_claim
    
    # 6. ìƒìœ„ Nê°œ ì„ ì •
    
    # (ì¸ë±ìŠ¤, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    weighted_scores = list(enumerate(sim_weighted))
    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_scores = sorted(weighted_scores, key=lambda item: item[1], reverse=True)
    
    top_candidates = []
    # ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒìœ„ Nê°œ ì¶”ì¶œ
    for index, score in sorted_scores: 
        candidate_id = db_ids[index]
        
        # [ì¤‘ìš”] ìê¸° ìì‹  ì œì™¸ (DB ì¿¼ë¦¬ì—ì„œ ì´ë¯¸ ì œì™¸í–ˆì§€ë§Œ, ì´ì¤‘ í™•ì¸)
        if candidate_id == submission_id:
            continue
            
        candidate_summary_json_str = db_summaries_json[index] # JSON ë¬¸ìì—´
        candidate_filename = db_original_filenames[index] # (í•„ìš”ì‹œ ì‚¬ìš© ê°€ëŠ¥)
        
        top_candidates.append({
            "candidate_id": candidate_id,
            "weighted_similarity": score,
            "candidate_summary_json_str": candidate_summary_json_str, # 4ë‹¨ê³„ ë¹„êµë¥¼ ìœ„í•´ ìš”ì•½ë³¸ ì›ë³¸ ì „ë‹¬
            "candidate_filename": candidate_filename
        })
        
        if len(top_candidates) >= top_n:
            break # ìƒìœ„ Nê°œë§Œ ì„ íƒ

    print(f"[find_similar_documents] ìƒìœ„ {len(top_candidates)}ê°œ í›„ë³´ ë°˜í™˜ ì™„ë£Œ.")
    return top_candidates

# ----------------------------------------------------
# --- 3. ë©”ì¸ ì„œë¹„ìŠ¤ í•¨ìˆ˜ (app.pyì—ì„œ í˜¸ì¶œ) ---
# ----------------------------------------------------

def perform_full_analysis_and_comparison(report_id, text, original_filename, json_prompt_template, comparison_prompt_template):
    """
    [ìˆ˜ì •] ì „ì²´ ë¶„ì„ ë° ë¹„êµ íŒŒì´í”„ë¼ì¸ (ìƒˆ í”„ë¡¬í”„íŠ¸, ìƒˆ ì„ë² ë”©, DB ì—°ë™)
    """
    
    # 0. ëª¨ë¸ ë¡œë“œ í™•ì¸
    if not llm_client_analysis or not embedding_model:
        print("[Service Analysis] CRITICAL: Service dependencies (LLM, S-BERT) not loaded.")
        raise Exception("LLM or Embedding model not loaded.")
    try:
        print(f"[{report_id}] Starting full analysis and comparison...")
        
        submission_analysis_json = _llm_call_analysis(
            raw_text=text,
            system_prompt=json_prompt_template
        )
        if not submission_analysis_json:
            raise Exception("LLM_ANALYSIS_FAILED: ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print(f"[{report_id}] 1. LLM ë¶„ì„ ì„±ê³µ.")   
    
    except Exception as e:
        print(f"[{report_id}] ğŸš¨ 5. [FATAL] LLM ë¶„ì„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ!")
        print(f"[{report_id}] ğŸš¨ ì˜¤ë¥˜ ë‚´ìš©: {str(e)}")
        traceback.print_exc() # <-- [í•„ìˆ˜] ì½˜ì†”ì— ì •í™•í•œ ì˜¤ë¥˜ ìœ„ì¹˜(ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤) ì¶œë ¥

        # (DB ì„¸ì…˜ ë¡¤ë°±)
        db.session.rollback()

        # (í”„ë¡ íŠ¸ì—”ë“œê°€ ë¬´í•œ ë¡œë”©ì— ë¹ ì§€ì§€ ì•Šë„ë¡ ìƒíƒœë¥¼ 'error'ë¡œ ì—…ë°ì´íŠ¸)
        try:
            if report: # report ê°ì²´ê°€ ì¡´ì¬í•˜ë©´
                report.status = 'error'
                report.error_message = str(e)[:1000] # ì˜¤ë¥˜ ë©”ì‹œì§€ ì €ì¥
                db.session.commit()
                print(f"[{report_id}] 6. DB ìƒíƒœ 'error'ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
            else:
                print(f"[{report_id}] 6. [Error] Report ê°ì²´ê°€ ì—†ì–´ DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨.")
        except Exception as db_err:
            # (DB ì—°ê²° ìì²´ê°€ ëŠê²¼ì„ ìµœì•…ì˜ ê²½ìš°)
            print(f"[{report_id}] ğŸš¨ 7. [FATAL] 'error' ìƒíƒœ ì—…ë°ì´íŠ¸ë§ˆì € ì‹¤íŒ¨: {db_err}")
            db.session.rollback()


    
    
    # --- 2ë‹¨ê³„: 2ê°œì˜ ì„ë² ë”© ìƒì„± (ì‹ ê·œ 0.6:0.4 ë¡œì§) ---
    print(f"[{report_id}] 2. ì„ë² ë”© ìƒì„± ì‹œì‘...")
    try:
        text_for_thesis = build_concat_text(
            submission_analysis_json.get('key_concepts', ''),
            submission_analysis_json.get('Core_Thesis', '')
        )
        text_for_claim = build_concat_text(
            submission_analysis_json.get('key_concepts', ''),
            submission_analysis_json.get('Claim', '')
        )
        embedding_thesis = get_embedding_vector(text_for_thesis) # (list)
        embedding_claim = get_embedding_vector(text_for_claim) # (list)

        if not embedding_thesis or not embedding_claim:
            raise Exception("EMBEDDING_FAILED: 1ê°œ ì´ìƒì˜ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"[{report_id}] 2. ì„ë² ë”© ìƒì„± ì„±ê³µ.")
            
    except Exception as e:
        print(f"[{report_id}] 2. ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise # 2ë‹¨ê³„ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨

    # --- 3ë‹¨ê³„: ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (DB ì¿¼ë¦¬) ---
    print(f"[{report_id}] 3. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ê°€ì¤‘í•© 0.6:0.4) ì‹œì‘...")
    candidate_docs = find_similar_documents(
        report_id, # [ì‹ ê·œ] ìê¸° ìì‹ ì„ ì œì™¸í•˜ê¸° ìœ„í•´ ID ì „ë‹¬
        embedding_thesis, 
        embedding_claim, 
        top_n=3
    )

    # --- 4ë‹¨ê³„: í›„ë³´ ë¬¸ì„œì™€ LLM ì •ë°€ ë¹„êµ (config.pyì˜ COMPARISON_SYSTEM_PROMPT ì‚¬ìš©) ---
    print(f"[{report_id}] 4. LLM ì •ë°€ ë¹„êµ (í›„ë³´ {len(candidate_docs)}ê°œ) ì‹œì‘...")
    comparison_results_list = []
    
    submission_json_str = json.dumps(submission_analysis_json, ensure_ascii=False) # ë¹„êµìš© (ì œì¶œë³¸ JSON ë¬¸ìì—´)

    for candidate in candidate_docs:
        try:
            candidate_id = candidate["candidate_id"]
            candidate_summary_str = candidate["candidate_summary_json_str"] # DBì˜ JSON ë¬¸ìì—´
            candidate_filename = candidate["candidate_filename"]
            print(f"  -> Comparing with: {candidate_id}")
            
            # LLM ë¹„êµ í˜¸ì¶œ
            comparison_report_text = _llm_call_comparison(
                submission_json_str, 
                candidate_summary_str,
                comparison_prompt_template
            )

            if comparison_report_text:
                comparison_results_list.append({
                    "candidate_id": candidate_id,
                    "candidate_filename" : candidate_filename,
                    "weighted_similarity": candidate['weighted_similarity'],
                    "llm_comparison_report": comparison_report_text # (6ê°œ ì ìˆ˜ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸)
                })
            else:
                 print(f"  -> WARNING: LLM (Comparison) failed for {candidate_id}.")
            
            sleep(1) # API ì†ë„ ì¡°ì ˆ

        except Exception as e:
            print(f"[{report_id}] 4. í›„ë³´ {candidate_id} ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")

    # --- 5. ìµœì¢… ë°ì´í„° ë°˜í™˜ (app.pyì˜ background_analysis_step1ì´ ë°›ì„ í˜•ì‹) ---
    analysis_data = {
        'summary_json': submission_analysis_json,      # (dict)
        'embedding_thesis': embedding_thesis,         # (list)
        'embedding_claim': embedding_claim,           # (list)
        'comparison_results_list': comparison_results_list # (list of dicts)
    }
    
    print(f"[{report_id}] 5. ëª¨ë“  ë¶„ì„ ì™„ë£Œ. ë°ì´í„° ë°˜í™˜.")
    return analysis_data


def _parse_comparison_scores(report_text):
    scores = {
        "Core Thesis": 0, "Problem Framing": 0, "Claim": 0,
        "Reasoning": 0, "Flow Pattern": 0, "Conclusion Framing": 0,
    }
    total_score = 0
    parsed_count = 0
    key_mapping = {
        "Core Thesis": "Core Thesis", "Problem Framing": "Problem Framing",
        "Claim": "Claim", "Reasoning": "Reasoning",
        "Flow Pattern": "Flow Pattern", "Conclusion Framing": "Conclusion Framing",
    }
    try:
        for key_name, mapped_key in key_mapping.items():
            pattern = rf"{re.escape(key_name)}.*?(?:Similarity):\s*(?:\*\*)?\s*(\d)(?:\*\*)?\s*[â€“-]"
            match = re.search(pattern, report_text, re.IGNORECASE | re.DOTALL)
            if match:
                score = int(match.group(1))
                scores[mapped_key] = score
                parsed_count += 1
            else:
                print(f"[_parse_comparison_scores] DEBUG: Failed to parse score for key: '{key_name}'")
        if parsed_count < 6:
            print(f"[_parse_comparison_scores] WARNING: Parsed {parsed_count}/6 scores.")
        scores["Core Thesis"] = scores["Core Thesis"] * 3
        scores["Claim"] = scores["Claim"] * 3
        scores["Reasoning"] = scores["Reasoning"] * 2
        scores["Flow Pattern"] = scores["Flow Pattern"] * 1
        scores["Problem Framing"] = scores["Problem Framing"] * 1
        scores["Conclusion Framing"] = scores["Conclusion Framing"] * 0
        total_score = sum(scores.values())
    except Exception as e:
        print(f"[_parse_comparison_scores] íŒŒì‹± ì¤‘ ì—ëŸ¬: {e}")
        return 0, scores
    return total_score, scores


def _filter_high_similarity_reports(comparison_results_list):
    high_similarity_reports = []
    threshold = 20
    for result in comparison_results_list:
        report_text = result.get("llm_comparison_report", "")
        total_score, scores_dict = _parse_comparison_scores(report_text)
        if total_score >= threshold:
            result['plagiarism_score'] = total_score
            result['scores_detail'] = scores_dict
            high_similarity_reports.append(result)
    return high_similarity_reports