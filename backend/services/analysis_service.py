import os
import json
import re
import numpy as np
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from time import sleep
import traceback # 1ë‹¨ê³„ ì˜¤ë¥˜ í•¸ë“¤ë§ì„ ìœ„í•´ ì¶”ê°€
import math
from extensions import db
from models import AnalysisReport

# --------------------------------------------------------------------------------------
# --- 1. ì „ì—­ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ (Flask ì•± ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰) ---
# --------------------------------------------------------------------------------------

GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
MAX_RETRIES = 3

# [ëª¨ë¸ ì„¤ì •]
ANALYSIS_MODEL_NAME = 'gemini-2.5-flash' # 1ë‹¨ê³„ ë¶„ì„/ìš”ì•½ìš©
COMPARISON_MODEL_NAME = 'gemini-2.5-flash' # 3ë‹¨ê³„ ë¹„êµìš©
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2' # 2ë‹¨ê³„ S-BERTìš©

# [LLM í´ë¼ì´ì–¸íŠ¸]
llm_client_analysis = None
llm_client_comparison = None

if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        llm_client_analysis = genai.GenerativeModel(ANALYSIS_MODEL_NAME)
        llm_client_comparison = genai.GenerativeModel(COMPARISON_MODEL_NAME)
        print(f"[Service Analysis] LLM Models ({ANALYSIS_MODEL_NAME}, {COMPARISON_MODEL_NAME}) loaded.")
    except Exception as e:
        print(f"[Service Analysis] CRITICAL: Gemini Client failed to load: {e}")
else:
    print("[Service Analysis] WARNING: GEMINI_API_KEY not found. LLM Analysis will fail.")

# [S-BERT ëª¨ë¸]
try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print(f"[Service Analysis] Embedding Model '{EMBEDDING_MODEL_NAME}' loaded.")
except Exception as e:
    print(f"[Service Analysis] CRITICAL: Failed to load SentenceTransformer model: {e}")
    embedding_model = None

# [DB ì„¤ì •]
print("[Service Analysis] Ready. (DB will be accessed via Flask context)")

# ----------------------------------------------------
# --- 2. í—¬í¼ í•¨ìˆ˜ ì •ì˜ (ë‚´ë¶€ìš©) ---
# ----------------------------------------------------

def _llm_call_analysis(raw_text, system_prompt):
    """(1ë‹¨ê³„ ë¶„ì„ìš©) Gemini ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ JSON êµ¬ì¡°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."""
    if not llm_client_analysis: return None
    
    config = genai.GenerationConfig(response_mime_type="text/plain") 
    
    prompt_content = f"{system_prompt}\n\nTarget Text: \n\n{raw_text[:10000]}..."
    
    for attempt in range(MAX_RETRIES):
        try:
            response = llm_client_analysis.generate_content(
                contents=[prompt_content], 
                generation_config=config
            )
            if not response.text: raise Exception("Empty response (Analysis)")
            
            match = re.search(r"```json\s*([\s\S]+?)\s*```", response.text)
            if not match:
                print(f"[Service Analysis] LLM_ANALYSIS_FAILED: JSON í˜•ì‹ ì‘ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Raw: {response.text[:200]}...")
                raise Exception("JSON format not found in LLM response.")
            
            return json.loads(match.group(1)) # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        
        except Exception as e:
            print(f"[Service Analysis] LLM Call Error (Attempt {attempt + 1}): {e}")
            if attempt < MAX_RETRIES - 1: sleep(2**attempt)
            else: print(f"[Service Analysis] Final Error (Analysis): {e}")
    return None

def _llm_call_comparison(submission_json_str, candidate_json_str, system_prompt_template):
    """(3ë‹¨ê³„ ë¹„êµìš©) Gemini ëª¨ë¸ë¡œ ë‘ JSONì„ 1:1 ë¹„êµí•©ë‹ˆë‹¤."""
    if not llm_client_comparison: return None
    
    user_prompt = system_prompt_template.format(
        submission_json_str=submission_json_str,
        candidate_json_str=candidate_json_str
    )
    
    for attempt in range(MAX_RETRIES):
        try:
            response = llm_client_comparison.generate_content(contents=[user_prompt])
            if not response.text: raise Exception("Empty response (Comparison)")
            return response.text # 6ê°œ í•­ëª© ì ìˆ˜ í…ìŠ¤íŠ¸ ë°˜í™˜
        
        except Exception as e:
            print(f"[Service Analysis] LLM Call Error (Attempt {attempt + 1}): {e}")
            if attempt < MAX_RETRIES - 1: sleep(2**attempt)
            else: print(f"[Service Analysis] Final Error (Comparison): {e}")
    return None

def get_embedding_vector(text):
    """[ì‹ ê·œ] í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë²¡í„°(list)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not embedding_model:
        print("[get_embedding_vector] ERROR: ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    try:
        vector = embedding_model.encode(text)
        return vector.tolist() # DB ì €ì¥ì„ ìœ„í•´ listë¡œ ë³€í™˜
    except Exception as e:
        print(f"[get_embedding_vector] ERROR: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def build_concat_text(key_concepts, main_idea):
    """[ì‹ ê·œ] ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¡°í•© (0.6:0.4 ë¡œì§ ê¸°ë°˜)"""
    return f"ì£¼ìš” ê°œë…: {key_concepts}\ní•µì‹¬ ì•„ì´ë””ì–´: {main_idea}"

def find_similar_documents(submission_id, sub_thesis_vec, sub_claim_vec, top_n=3):
    """
    [Full Code]
    í•­ìƒ ì‹¤ì‹œê°„ SQL DB (AnalysisReport)ë¥¼ ì¿¼ë¦¬í•©ë‹ˆë‹¤.
    is_test=Falseì¸ ë¦¬í¬íŠ¸ë§Œ ë¹„êµ ëŒ€ì¡°êµ°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    try:
        sub_thesis_np = np.array(sub_thesis_vec).reshape(1, -1)
        sub_claim_np = np.array(sub_claim_vec).reshape(1, -1)
    except Exception as e:
        print(f"[find_similar_documents] ERROR: ì œì¶œëœ ë²¡í„°ë¥¼ NumPyë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return []
        
    db_ids = []
    db_summaries_json = [] 
    db_vectors_thesis_list = []
    db_vectors_claim_list = []
    db_original_filenames = []

    db_vectors_thesis_np = None
    db_vectors_claim_np = None

    print(f"[find_similar_documents] Using Live DB Query (is_test=False, Excluding ID: {submission_id})")
    try:
        query = AnalysisReport.query.filter(
            AnalysisReport.embedding_keyconcepts_corethesis.isnot(None),
            AnalysisReport.embedding_keyconcepts_claim.isnot(None),
            AnalysisReport.is_test == False
        )
        
        if submission_id:
             query = query.filter(AnalysisReport.id != submission_id)
        
        all_reports = query.all()

        if not all_reports:
            print("[find_similar_documents] ë¹„êµí•  DB ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. (is_test=False í•„í„°ë§ë¨)")
            return []

        for report in all_reports:
            try:
                db_ids.append(report.id)
                db_summaries_json.append(report.summary) 
                db_original_filenames.append(report.original_filename)
                vec_thesis = json.loads(report.embedding_keyconcepts_corethesis)
                vec_claim = json.loads(report.embedding_keyconcepts_claim)
                
                db_vectors_thesis_list.append(vec_thesis)
                db_vectors_claim_list.append(vec_claim)
                
            except Exception as e:
                print(f"[find_similar_documents] Report {report.id} ì„ë² ë”©/ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        if not db_ids:
            print("[find_similar_documents] ìœ íš¨í•œ DB ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        db_vectors_thesis_np = np.array(db_vectors_thesis_list)
        db_vectors_claim_np = np.array(db_vectors_claim_list)

    except Exception as e:
        print(f"[find_similar_documents] CRITICAL: Live DB ì¿¼ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

    if db_vectors_thesis_np is None or db_vectors_claim_np.shape[0] == 0:
        print("[find_similar_documents] DB ë²¡í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []
        
    sim_thesis = cosine_similarity(sub_thesis_np, db_vectors_thesis_np)[0]
    sim_claim = cosine_similarity(sub_claim_np, db_vectors_claim_np)[0]

    WEIGHT_THESIS = 0.6
    WEIGHT_CLAIM = 0.4
    sim_weighted = WEIGHT_THESIS * sim_thesis + WEIGHT_CLAIM * sim_claim
    
    weighted_scores = list(enumerate(sim_weighted))
    sorted_scores = sorted(weighted_scores, key=lambda item: item[1], reverse=True)
    
    top_candidates = []
    for index, score in sorted_scores: 
        candidate_id = db_ids[index]
        
        if candidate_id == submission_id:
            continue
            
        candidate_summary_json_str = db_summaries_json[index] 
        candidate_filename = db_original_filenames[index] 
        
        top_candidates.append({
            "candidate_id": candidate_id,
            "weighted_similarity": score,
            "candidate_summary_json_str": candidate_summary_json_str, 
            "candidate_filename": candidate_filename
        })
        
        if len(top_candidates) >= top_n:
            break 

    print(f"[find_similar_documents] ìƒìœ„ {len(top_candidates)}ê°œ í›„ë³´ ë°˜í™˜ ì™„ë£Œ.")
    return top_candidates

# ----------------------------------------------------
# --- 3. ë©”ì¸ ì„œë¹„ìŠ¤ í•¨ìˆ˜ (app.pyì—ì„œ í˜¸ì¶œ) ---
# ----------------------------------------------------

def perform_step1_analysis_and_embedding(report_id, text, json_prompt_template):
    """
    [ì‹ ê·œ] 1ë‹¨ê³„: LLM ë¶„ì„ ë° ì„ë² ë”© ìƒì„±
    (app.pyì˜ background_analysis_step1_analysisì—ì„œ í˜¸ì¶œ)
    """
    
    # 0. ëª¨ë¸ ë¡œë“œ í™•ì¸
    if not llm_client_analysis or not embedding_model:
        print("[Service Analysis] CRITICAL: Service dependencies (LLM, S-BERT) not loaded.")
        raise Exception("LLM or Embedding model not loaded.")
    
    print(f"[{report_id}] Starting Step 1: Analysis and Embedding...")
    
    # --- 1ë‹¨ê³„: LLM ë¶„ì„ ---
    submission_analysis_json = _llm_call_analysis(
        raw_text=text,
        system_prompt=json_prompt_template
    )
    if not submission_analysis_json:
        raise Exception("LLM_ANALYSIS_FAILED: ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    print(f"[{report_id}] 1. LLM ë¶„ì„ ì„±ê³µ.") 
    
    # --- 2ë‹¨ê³„: 2ê°œì˜ ì„ë² ë”© ìƒì„± (ì‹ ê·œ 0.6:0.4 ë¡œì§) ---
    print(f"[{report_id}] 2. ì„ë² ë”© ìƒì„± ì‹œì‘...")
    try:
        text_for_thesis = build_concat_text(
            submission_analysis_json.get('key_concepts', ''),
            submission_analysis_json.get('Core_Thesis', '')
        )
        text_for_claim = build_concat_text(
            submission_analysis_json.get('key_concepts', ''),
            submission_analysis_json.get('Claim', '')
        )
        embedding_thesis = get_embedding_vector(text_for_thesis) # (list)
        embedding_claim = get_embedding_vector(text_for_claim) # (list)

        if not embedding_thesis or not embedding_claim:
            raise Exception("EMBEDDING_FAILED: 1ê°œ ì´ìƒì˜ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"[{report_id}] 2. ì„ë² ë”© ìƒì„± ì„±ê³µ.")
            
    except Exception as e:
        print(f"[{report_id}] 2. ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise # 2ë‹¨ê³„ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨

    # --- 3. 1ë‹¨ê³„ ë°ì´í„° ë°˜í™˜ ---
    analysis_data = {
        'summary_json': submission_analysis_json,      # (dict)
        'embedding_thesis': embedding_thesis,          # (list)
        'embedding_claim': embedding_claim,            # (list)
    }
    
    print(f"[{report_id}] Step 1 (Analysis & Embedding) ì™„ë£Œ. ë°ì´í„° ë°˜í™˜.")
    return analysis_data


def perform_step2_comparison(report_id, embedding_thesis, embedding_claim, submission_json_str, comparison_prompt_template):
    """
    [ì‹ ê·œ] 2ë‹¨ê³„: ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ë° LLM ì •ë°€ ë¹„êµ
    (app.pyì˜ background_analysis_step2_comparisonì—ì„œ í˜¸ì¶œ)
    """

    if not llm_client_comparison:
        print("[Service Analysis] CRITICAL: Comparison LLM not loaded.")
        raise Exception("Comparison LLM not loaded.")

    print(f"[{report_id}] Starting Step 2: Comparison...")

    # --- 3ë‹¨ê³„: ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (DB ì¿¼ë¦¬) ---
    print(f"[{report_id}] 3. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ê°€ì¤‘í•© 0.6:0.4) ì‹œì‘...")
    candidate_docs = find_similar_documents(
        report_id, 
        embedding_thesis, 
        embedding_claim, 
        top_n=3
    )

    # --- 4ë‹¨ê³„: í›„ë³´ ë¬¸ì„œì™€ LLM ì •ë°€ ë¹„êµ ---
    print(f"[{report_id}] 4. LLM ì •ë°€ ë¹„êµ (í›„ë³´ {len(candidate_docs)}ê°œ) ì‹œì‘...")
    comparison_results_list = []
    
    for candidate in candidate_docs:
        try:
            candidate_id = candidate["candidate_id"]
            candidate_summary_str = candidate["candidate_summary_json_str"] # DBì˜ JSON ë¬¸ìì—´
            candidate_filename = candidate["candidate_filename"]
            print(f"  -> Comparing with: {candidate_id}")
            
            # LLM ë¹„êµ í˜¸ì¶œ
            comparison_report_text = _llm_call_comparison(
                submission_json_str, 
                candidate_summary_str,
                comparison_prompt_template
            )

            if comparison_report_text:
                comparison_results_list.append({
                    "candidate_id": candidate_id,
                    "candidate_filename" : candidate_filename,
                    "weighted_similarity": candidate['weighted_similarity'],
                    "llm_comparison_report": comparison_report_text # (6ê°œ ì ìˆ˜ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸)
                })
            else:
                 print(f"  -> WARNING: LLM (Comparison) failed for {candidate_id}.")
            
            sleep(1) # API ì†ë„ ì¡°ì ˆ

        except Exception as e:
            print(f"[{report_id}] 4. í›„ë³´ {candidate_id} ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"[{report_id}] Step 2 (Comparison) ì™„ë£Œ. ë¹„êµ ê²°ê³¼ ë°˜í™˜.")
    return comparison_results_list # (list of dicts)


# --- (ê¸°ì¡´ perform_full_analysis_and_comparison í•¨ìˆ˜ëŠ” ì‚­ì œë¨) ---


def _parse_comparison_scores(report_text):
    scores = {
        "Core Thesis": 0, "Problem Framing": 0, "Claim": 0,
        "Reasoning": 0, "Flow Pattern": 0, "Conclusion Framing": 0,
    }
    total_score = 0
    parsed_count = 0
    key_mapping = {
        "Core Thesis": "Core Thesis", "Problem Framing": "Problem Framing",
        "Claim": "Claim", "Reasoning": "Reasoning",
        "Flow Pattern": "Flow Pattern", "Conclusion Framing": "Conclusion Framing",
    }
    try:
        for key_name, mapped_key in key_mapping.items():
            # ì ìˆ˜ íŒŒì‹± ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
            pattern = rf"{re.escape(key_name)}.*?(?:Similarity):\s*(?:\*\*)?\s*(\d)(?:\*\*)?\s*[â€“-]"
            match = re.search(pattern, report_text, re.IGNORECASE | re.DOTALL)
            if match:
                score = int(match.group(1))
                scores[mapped_key] = score
                parsed_count += 1
            else:
                print(f"[_parse_comparison_scores] DEBUG: Failed to parse score for key: '{key_name}'")
        
        if parsed_count < 6:
            print(f"[_parse_comparison_scores] WARNING: Parsed {parsed_count}/6 scores.")

        # 2. ìƒˆë¡œìš´ ì ìˆ˜ ë³€í™˜ ë¡œì§ ì ìš©

        # Core Thesis: (ì ìˆ˜ - 8, ìŒìˆ˜ë©´ 0)ì˜ ì œê³± * 2    
        original_ct = scores["Core Thesis"]
        scores["Core Thesis"] = max(0, original_ct - 8) ** 2 * 2
        
        # Claim: (ì ìˆ˜ - 8, ìŒìˆ˜ë©´ 0)ì˜ ì œê³± * 2   
        original_claim = scores["Claim"]
        scores["Claim"] = max(0, original_claim - 8) ** 2 * 2

        # Reasoning: (ì ìˆ˜ - 5, ìŒìˆ˜ë©´ 0)ì˜ 1.5ìŠ¹ * 2 ë¥¼ ì •ìˆ˜ ì²˜ë¦¬ 
        original_reasoning = scores["Reasoning"]
        scores["Reasoning"] = int(math.pow(max(0, original_reasoning - 5), 1.5) * 2)

        # Flow Pattern: (ì ìˆ˜ - 6, ìŒìˆ˜ë©´ 0)ì˜ ì œê³± * 2  
        original_fp = scores["Flow Pattern"]
        scores["Flow Pattern"] = max(0, original_fp - 6) ** 2 * 2
        
        # Problem Framing: (ì ìˆ˜ - 5, ìŒìˆ˜ë©´ 0) * 2   
        original_pf = scores["Problem Framing"]
        scores["Problem Framing"] = max(0, original_pf - 5) * 2

        # Conclusion Framing: (ì ìˆ˜ - 5, ìŒìˆ˜ë©´ 0) * 2  
        original_cf = scores["Conclusion Framing"]
        scores["Conclusion Framing"] = max(0, original_cf - 5) * 2
        
        # 3. ì´ì  ê³„ì‚°
        total_score_converted = sum(scores.values())
        
        # 4. 100ì  ë§Œì ìœ¼ë¡œ í™˜ì‚° í›„ ì •ìˆ˜ ì²˜ë¦¬
        if MAX_TOTAL_SCORE > 0:
            # ğŸ“Œ ìµœì¢… ì ìˆ˜ì— int() ì ìš©
            final_score_100 = int((total_score_converted / MAX_TOTAL_SCORE) * 100)
        else:
            final_score_100 = 0
            
    except Exception as e:
        print(f"[_parse_comparison_scores] íŒŒì‹± ì¤‘ ì—ëŸ¬: {e}")
        return 0, scores
    
    return final_score_100, scores



def _filter_high_similarity_reports(comparison_results_list):
    high_similarity_reports = []
    threshold = 60
    for result in comparison_results_list:
        report_text = result.get("llm_comparison_report", "")
        total_score, scores_dict = _parse_comparison_scores(report_text)
        if total_score >= threshold:
            result['plagiarism_score'] = total_score
            result['scores_detail'] = scores_dict
            high_similarity_reports.append(result)
    return high_similarity_reports
